<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Media Content Atlas (MCA) is a first-of-its-kind pipeline that enables large-scale, AI-driven analysis of digital media experiences using multimodal LLMs.">
  <meta property="og:title" content="Media Content Atlas"/>
  <meta property="og:description" content="A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs"/>
  <meta property="og:url" content="mediacontentatlas.github.io"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/mca-interface.png" />


  <meta name="twitter:title" content="Media Content Atlas">
  <meta name="twitter:description" content="A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/mca-interface.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Media Content Atlas (MCA) is a first-of-its-kind pipeline that enables large-scale, AI-driven analysis of digital media experiences using multimodal LLMs.">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Media Content Atlas</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>📲🗺️</text></svg>">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Media Content Atlas</h1>
            <h2 class="subtitle is-3">A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=-ziwkNEAAAAJ" target="_blank">Merve Cerit</a>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=en&user=V5B8dSUAAAAJ" target="_blank">Eric Zelikman</a>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=en&user=A_NOSQsAAAAJ" target="_blank">Mu-Jung Cho</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=en&user=UuH5OG8AAAAJ" target="_blank">Thomas N. Robinson</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=en&user=HJWSrNMAAAAJ" target="_blank">Byron Reeves</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=V6mIZuYAAAAJ&hl=en&oi=ao" target="_blank">Nilam Ram</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=euNCoVYAAAAJ&hl=en&oi=ao" target="_blank">Nick Haber</a></span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Stanford University<br>Accepted to <a href="https://chi2025.acm.org" target="_blank">CHI 2025</a></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/mediacontentatlas/mediacontentatlas" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="hero-body">
      <div style="width: 75%; margin: 0 auto;">
          <div id="media-container"></div>
      </div>
      <div class="container is-max-desktop">
      <div class="box has-background-info-light has-text-centered mt-5 mb-5">
        <h2 class="title is-4 mb-0">
          <strong>How can researchers study complex, real-world digital media experiences at scale?</strong>
        </h2>
      </div>
      <p class="has-text-justified">
        We introduce the <strong>Media Content Atlas (MCA)</strong>—a <strong>customizable, AI-powered pipeline and interactive dashboard</strong> for exploring and investigating digital media content across platforms at scale.  
  Each point in the dashboard represents a <strong>single captured screen</strong>—a moment of what a participant sees and interacts with on their smartphone. 
  This data comes from <strong>1.12 million screenshots</strong> collected from <strong>112 participants</strong> over the course of <strong>one month</strong>, 
  sampled every <strong>5 seconds</strong> while screens were on. Topics are color-coded and <strong>automatically identified</strong> by MCA’s AI-driven pipeline.
      </p>
      <div class="box has-background-light mt-5 mb-5">
        <p class="is-size-5 has-text-weight-semibold mb-3">🧭 Media Content Atlas enables researchers to:</p>
        <ul class="ml-4">
          <li>📊 <strong>Capture and analyze</strong> moment-by-moment screen content</li>
          <li>🔍 <strong>Cluster</strong> media by semantic and visual similarity</li>
          <li>📑 <strong>Automate</strong> topic modeling and labeling</li>
          <li>🖼️ <strong>Retrieve</strong> content from theoretical or conceptual queries</li>
          <li>🎛️ <strong>Visualize and explore</strong> media landscapes interactively</li>
        </ul>
      </div>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper Background -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">  
    <div class="column is-four-fifths">
        <h2 class="title is-3">The Problem with Traditional Approaches</h2>
        <div class="container is-max-desktop">
        <div class="content has-text-justified">
          <p>
            Screens are deeply embedded in how we live, learn, and connect—yet research often reduces digital experience to <strong>app-based metrics</strong> like <strong>time spent per app</strong>.
          </p>
          </div>
          <div class="content has-text-left">
          <div class="box has-background-warning-light">
            <p class="is-size-5 has-text-weight-semibold mb-3"> What App-Based Metrics Miss:</p>
            <p class="mb-4">
              <strong>📱 What users actually see:</strong> An Instagram feed of makeup tutorials vs. cyberbullying is treated the same, despite vastly different effects.
            </p>

            <p class="mb-4">
              <strong>🔄 How content flows across apps:</strong> Users jump between apps and browsers for related content, but this cross-platform flow is often overlooked.
            </p>

            <p>
              <strong>🌱 Emergent behaviors:</strong> App categories are rigid and predefined. They don’t adapt to the diverse ways users engage with content—or how those behaviors evolve over time.
            </p>
          </div>
        </div>
        <div class="content has-text-justified">
          <p>
            To address these blind spots, recent research has turned to <strong>continuous screen data collection</strong>, which captures <strong>everything displayed on users' screens</strong>.
          </p>
          <div class="box has-background-light mt-4">
            <p>
              Despite these advances, <strong>there are still no scalable and flexible tools</strong> that allow researchers to <strong>interact with, explore, and analyze massive, sensitive, and unstructured multimodal data</strong>.
            </p>
          </div>
          </div>
        </div>
    </div>
  </div>
</section>
<!-- End paper background -->
<!-- Paper Data -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Beyond App Names: Capturing Media Content via Screenomics</h2>
        <div class="container is-max-desktop">
        <div class="content has-text-justified">
          <p>
            To design and evaluate Media Content Atlas, we used <strong>1.12 million screens</strong> (10,000 screenshots per participant) collected from <strong>112 participants</strong>  over the course of <strong></strong>one month</strong>.  
            This data comes from the <span><a href="https://screenomics.stanford.edu">Human Screenome Project</a></span>, where a study-specific app captured a screenshot <strong>every 5 seconds</strong> whenever the screen was on, <strong>creating a continuous, moment-by-moment record of digital experiences</strong>.
          </p>
          </div>
          <div class="box has-background-light mt-4 mb-4 has-text-left">
            <h3 class="title is-5 mb-2">🔎 The Scale Challenge</h3>
            <p>
              Manually reviewing this dataset would take <strong>190+ workdays</strong>, <strong>making AI-augmented exploration and analysis essential</strong>.
            </p>
          </div>
          <div class="content has-text-centered">
            <img src="static/videos/screenome.gif" alt="Media Content Atlas GIF" style="max-width: 100%; height: auto;">
            <p class="caption-text"><strong>Figure.</strong> A short slice of a Screenome. This 15-minute segment, recorded from a consenting researcher’s real-world smartphone use, illustrates how digital content and media experiences shift even over brief periods of time. Colors indicate different app categories, while the size of each bar represents the amount of time spent on each app category.</p>
            </p>
          </div>
          </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper Data -->
<script>
  window.addEventListener("scroll", () => {
    const appList = document.getElementById("appNames");
    const contentList = document.getElementById("contentTopics");
    const scrollY = window.scrollY;

    if (scrollY > 300) {
      appList.style.opacity = 0;
      appList.style.transform = "translateX(-20px)";
      contentList.style.opacity = 1;
      contentList.style.transform = "translateX(0)";
    } else {
      appList.style.opacity = 1;
      appList.style.transform = "translateX(0)";
      contentList.style.opacity = 0;
      contentList.style.transform = "translateX(-20px)";
    }
  });
</script>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths has-text-centered">
        <h2 class="title is-3">Media Content Atlas Pipeline</h2>
        <div class="content has-text-justified mt-4">
          <p>
            MCA processes screen data <strong>scalably and securely</strong> through a four-step pipeline: embedding, clustering, retrieval, and interactive visualization. These steps allow researchers to <strong>transform raw screen data into structured, searchable, and explorable insights</strong>.
          </p>
          
        </div>

        <!-- MCA Pipeline Figure -->
        <div style="max-width: 720px; margin: 0rem auto;">
          <div style="box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05); border-radius: 8px; padding: 1rem;">
            <figure class="has-text-centered">
              <img 
                src="static/images/mca-pipeline2.png" 
                alt="Media Content Atlas Pipeline Overview" 
                style="max-width: 100%; height: auto; border-radius: 6px;">
              <p class="caption-text is-size-7 mt-3">
                <strong>Figure.</strong> MCA's end-to-end pipeline includes embedding and description generation, clustering and topic modeling, semantic image retrieval, and interactive visualization of media content.
              </p>
            </figure>
          </div>
        </div>
        <div class="content has-text-justified mt-4">
          <p>
            Built on <strong>open-source multimodal models</strong> (e.g., CLIP, LLaVA-One Vision), MCA requires <strong>no manual annotation</strong> and ensures <strong>data security</strong> by running on in-house servers.  
            Its <strong>model-agnostic architecture</strong> allows flexible customization via <strong>prompting and fine-tuning</strong>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Expert Evaluation -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Expert Evaluation & Findings</h2>

        <div class="columns is-vcentered">
          <!-- Left Column (Text) -->
          <div class="column is-half">
            <div class="content has-text-justified">

              <p>
                We conducted a structured evaluation of MCA with domain experts in communication, psychology, medicine, and education. 
                Each expert participated in <strong>think-aloud sessions</strong> and <strong>three hours of structured ratings</strong>, evaluating MCA’s clustering, topic labeling, and retrieval features.
              </p>
            </div>
              <!-- Evaluation Results -->
              <div class="box has-background-light">
                <p class="is-size-5 has-text-weight-semibold mb-3">✅ Key Evaluation Outcomes</p>
                <ul>
                  <li class="mb-2"><strong>Cluster Relevance:</strong> <strong>96%</strong> of topic labels were rated <em>relevant/highly relevant</em>.</li>
                  <li class="mb-2"><strong>Description Accuracy:</strong> <strong>83%</strong> of AI-generated descriptions were rated <em>accurate/highly accurate</em>.</li>
                  <li class="mb-2"><strong>Content Similarity:</strong> <strong>89%</strong> of clustered images were considered <em>semantically similar</em>.</li>
                  <li class="mb-2"><strong>Image Retrieval Performance:</strong> <strong>79.5%</strong> of retrieved images matched the <em>intended search concept</em>.</li>
                  <li><strong>Comparison with Traditional Methods:</strong> Experts unanimously found MCA <em>more informative and useful</em> than app-based categorization.</li>
                </ul>
              </div>
          </div>

          <!-- Right Column (Figure) -->
          <div class="column is-half has-text-centered">
          <div style="max-width: 360px; margin: 2rem auto;">
            <div style="box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05); border-radius: 8px; padding: 1rem;">
              <figure class="has-text-centered">
                <img src="static/images/allfigures.png" alt="Expert Ratings on MCA Clusters" style="max-width: 100%; height: auto;">
                <p class="caption-text is-size-7 mt-3">
                  <strong>Figure.</strong> <em>Expert ratings on cluster interpretability and relevance.</em> Bar plots (left) show label-level response counts. Boxplots (right) illustrate score distributions across clusters and domains.
                </p>
              </figure>
            </div>
          </div>
        </div>
        </div>
        <!-- Full-width figure for Image Retrieval -->
        <div style="max-width: 720px; margin: 0rem auto;">
          <div style="box-shadow: 0 2px 10px rgba(0, 0, 0, 0.05); border-radius: 8px; padding: 1rem;">
            <figure class="has-text-centered">
              <img 
                src="static/images/figure5.png" 
                alt="Expert Ratings on the Image Retrieval" 
                style="max-width: 100%; height: auto; border-radius: 6px;">
              <p class="caption-text is-size-7 mt-3">
                <strong>Figure.</strong> <em>Expert evaluation of image retrieval performance.</em> Experts rated the relevance of retrieved images to the intended search concept.
              </p>
            </figure>
          </div>
        </div>

          </div>
        </div>
      </div>
</section>

<!-- Contact Section -->
<section class="section has-text-centered has-background-light">
  <div class="container">
    <h2 class="title is-4">Get in Touch</h2>
    <p class="is-size6 mb-4">
      Have questions, ideas, or want to collaborate? Let’s connect.
    </p>

    <a href="mailto:mervecer@stanford.edu" class="button is-info is-small">
      <span class="icon">
        <i class="fas fa-envelope"></i>
      </span>
      <span>Send an Email</span>
    </a>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{cerit2025mca,
        author = {Merve Cerit and Eric Zelikman and Mu-Jung Cho and Thomas N. Robinson and Byron Reeves and Nilam Ram and Nick Haber},
        title = {Media Content Atlas: A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs},
        booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA '25)},
        year = {2025},
        month = {April},
        location = {Yokohama, Japan},
        publisher = {ACM},
        address = {New York, NY, USA},
        pages = {19},
        doi = {10.1145/3706599.3720055}
      }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->

<div class="section-divider">
  <img src="static/images/divider.png" alt="Color divider" style="width: 100%; height: 8px; max-width: 1800; margin: 0rem auto; display: block;">
</div>
<footer class="footer has-background-light">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">

          <p class="is-size-6 mb-3"><strong>Learn more about Media Content Atlas.</strong></p>
          <div class="buttons is-centered">
            <a href="https://arxiv.org/pdf/<ARXIV_PAPER_ID>.pdf" target="_blank" class="button is-dark is-small">
              <span class="icon"><i class="fas fa-file-pdf"></i></span>
              <span>CHI 2025 Paper</span>
            </a>
            <a href="https://github.com/mediacontentatlas/mediacontentatlas" target="_blank" class="button is-dark is-small">
              <span class="icon"><i class="fab fa-github"></i></span>
              <span>Code</span>
            </a>
            <a href="https://arxiv.org/abs/<ARXIV_PAPER_ID>" target="_blank" class="button is-dark is-small">
              <span class="icon"><i class="ai ai-arxiv"></i></span>
              <span>arXiv</span>
            </a>
          </div>

          <p class="is-size-7 mt-4">
            This page is based on the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.<br>
            Licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">CC BY-SA 4.0</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>
<script>
  function getDeviceType() {
    const ua = navigator.userAgent;
    const isTouch = 'ontouchstart' in window || navigator.maxTouchPoints > 0;

    const isMobileUA = /Mobi|Android|iPhone|iPod|Opera Mini|IEMobile/i.test(ua);
    const isTabletUA = /iPad|Tablet|Nexus 7|Nexus 10|KFAPWI/i.test(ua);
    const isSmallScreen = window.innerWidth <= 768;

    if (isMobileUA && isSmallScreen) return 'phone';
    if ((isTabletUA || isTouch) && window.innerWidth <= 1024) return 'tablet';
    return 'desktop';
  }

  function renderMediaElement() {
    const container = document.getElementById('media-container');
    const type = getDeviceType();

    if (type === 'desktop') {
      container.innerHTML = `
        <iframe src="static/interactive/MCA_topic_based_no_desc.html"
          width="100%" height="500px"
          style="border: 2px solid #eff5fb; border-radius: 10px;">
        </iframe>
      `;
    } else {
      container.innerHTML = `
        <video poster="" id="tree" autoplay controls muted loop style="width: 100%; height="500px", border-radius: 8px;">
          <source src="static/videos/MCAshort.mp4" type="video/mp4">
          Your device does not support video playback.
        </video>
      `;
    }
  }
  window.addEventListener('DOMContentLoaded', renderMediaElement);
</script>
</body>
</html>