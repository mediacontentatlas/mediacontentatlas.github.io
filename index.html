<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="Media Content Atlas (MCA) is a first-of-its-kind pipeline that enables large-scale, AI-driven analysis of digital media experiences using multimodal LLMs.">
  <meta property="og:title" content="Media Content Atlas"/>
  <meta property="og:description" content="A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs"/>
  <meta property="og:url" content="mediacontentatlas.github.io"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/mca-interface.png" />


  <meta name="twitter:title" content="Media Content Atlas">
  <meta name="twitter:description" content="A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/mca-interface.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Media Content Atlas (MCA) is a first-of-its-kind pipeline that enables large-scale, AI-driven analysis of digital media experiences using multimodal LLMs.">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Media Content Atlas</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üì≤üó∫Ô∏è</text></svg>">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Media Content Atlas</h1>
            <h2 class="subtitle is-3">A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=-ziwkNEAAAAJ" target="_blank">Merve Cerit</a>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=en&user=V5B8dSUAAAAJ" target="_blank">Eric Zelikman</a>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=en&user=A_NOSQsAAAAJ" target="_blank">Mu-Jung Cho</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=en&user=UuH5OG8AAAAJ" target="_blank">Thomas N. Robinson</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=en&user=HJWSrNMAAAAJ" target="_blank">Byron Reeves</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=V6mIZuYAAAAJ&hl=en&oi=ao" target="_blank">Nilam Ram</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=euNCoVYAAAAJ&hl=en&oi=ao" target="_blank">Nick Haber</a></span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Stanford University<br>Accepted to <a href="https://chi2025.acm.org" target="_blank">CHI 2025</a></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/mediacontentatlas/mediacontentatlas" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/MCAshort.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <strong>How can researchers study complex, real-world digital media experiences at scale?</strong>
      </h2>
      <p class="has-text-justified">
        We introduce the <strong>Media Content Atlas (MCA)</strong>‚Äîa customizable AI-powered pipeline and interactive dashboard for <strong>content-based clustering, topic modeling, image retrieval</strong>, and <strong>visualization</strong>.
      </p>
      <p class="has-text-justified">
        Each point in the dashboard represents a single screen‚Äîa moment in digital life. 
        The dataset includes <strong>1.12 million screens</strong> from <strong>112 participants</strong> over one month. 
        Colors indicate media content topics, <strong>automatically identified by MCA‚Äôs AI-driven pipeline.</strong>
      </p>
    </div>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper Background -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Why MCA? The Problem with Traditional Approaches</h2>
        <div class="content has-text-justified">
          <p>
            People spend <strong>over 8 hours a day on screens</strong>, yet research still relies on <strong>app-based metrics</strong> 
            like <strong>time spent per app</strong>, which fail to capture:
          </p>
          <ul>
            <li>üì± <strong>What users actually see:</strong> An Instagram feed of makeup tutorials vs. cyberbullying is treated the same, despite vastly different impacts.</li>
            <li>üîÑ <strong>How content flows across apps:</strong> Users switch between platforms for related content (e.g., reading news, then watching a follow-up video), but this cross-platform flow is lost.</li>
            <li>üìä <strong>Thematic insights:</strong> App categories are rigid and don‚Äôt reflect actual user experiences.</li>
          </ul>
          <p>
            To address this, recent research uses <strong>continuous screen data collection</strong> captures <strong>everything displayed on users' screens</strong>.
           </p>
           <p>
            Despite this, <strong>there are no scalable and flexible tools</strong> that allow researchers to <strong>interact with, explore, and analyze such massive, sensitive, and unstructured multimodal data</strong>.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper background -->
<!-- Paper Data -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Beyond App Names: Capturing Media Content Beyond App Names</h2>
        <div class="container is-max-desktop">
        <div class="content has-text-justified">
          <p>
            To design and evaluate Media Content Atlas, we used <strong>1.12 million screens</strong> (10,000 screenshots per participant) collected from <strong>112 participants</strong>  over the course of <strong></strong>one month</strong>.  
            This data comes from the <span><a href="https://screenomics.stanford.edu">Human Screenome Project</a></span>, where a study-specific app captured a screenshot <strong>every 5 seconds</strong> whenever the screen was on, creating a continuous, moment-by-moment record of digital experiences.
          </p>
          <p>
            üîé <strong>The Scale Challenge:</strong> Manually reviewing this dataset would take 190+ workdays, making AI-augmented exploration and analysis essential.
          </p>
          <div class="content has-text-centered">
            <img src="static/videos/screenome.gif" alt="Media Content Atlas GIF" style="max-width: 100%; height: auto;">
            <p class="caption-text"><strong>Figure.</strong> Example of MCA's input data.
            </p>
          </div>
            <p>
              The video above shows a 15-minute segment from one participant‚Äôs digital life, demonstrating how media experiences unfold in different content in real-time.
            </p>
          </div>
          </div>   
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper Data -->
<!-- Figures Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths has-text-centered">
        <h2 class="title is-3">Media Content Atlas Pipeline</h2>
        
        <!-- Large Figure -->
        <figure>
          <img src="static/images/mca-pipeline2.png" alt="Media Content Atlas Pipeline Overview">
          <div class="content has-text-justified">
            <p>MCA processes screen data <strong>scalably and securely</strong> through the following steps:</p>
            <ul>
              <li>1Ô∏è‚É£ <strong>Embedding & Description Generation</strong> ‚Äì Converts images into numerical representations and text descriptions.</li>
              <li>2Ô∏è‚É£ <strong>Clustering & Topic Modeling</strong> ‚Äì Organizes content into meaningful, interpretable topics.</li>
              <li>3Ô∏è‚É£ <strong>Image Retrieval</strong> ‚Äì Enables semantic search for specific media content.</li>
              <li>4Ô∏è‚É£ <strong>Interactive Visualization</strong> ‚Äì Facilitates dynamic exploration of digital experiences.</li>
            </ul>
          
            <p>
              Built on <strong>open-source multimodal models</strong> (e.g., CLIP, LLaVA-One Vision), MCA requires <strong>no manual annotation</strong> and ensures <strong>data security</strong> by running on in-house servers.  
              Its <strong>model-agnostic architecture</strong> allows customization via <strong>prompting and fine-tuning</strong>.
            </p>
        </figure>

      </div>
    </div>
  </div>
</section>
<!-- End Figures Section -->
<!-- Expert Evaluation -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Expert Evaluation & Findings</h2>
        <!-- Two-column layout for Clusters -->
        <div class="columns is-vcentered">
          <!-- Left Column (Text) -->
          <div class="column is-half">
            <p class="content has-text-justified">
              To assess MCA‚Äôs effectiveness, we conducted an evaluation with domain experts from 
              communication, psychology, medicine, and education. Through <strong>think-aloud sessions</strong> and 
              <strong>structured ratings</strong> (three hours per expert), they assessed MCA‚Äôs clustering, topic labeling, 
              and retrieval capabilities.<ul>
                <li>‚úÖ <strong>Cluster Relevance:</strong> <strong>96%</strong> of topic labels were rated <strong>relevant/highly relevant</strong>.</li>
                <li>‚úÖ <strong>Description Accuracy:</strong> <strong>83%</strong> of AI-generated descriptions were <strong>accurate/highly accurate</strong>.</li>
                <li>‚úÖ <strong>Content Similarity:</strong> <strong>89%</strong> of clustered images were considered <strong>semantically similar</strong>.</li>
                <li>‚úÖ <strong>Image Retrieval Performance:</strong> <strong>79.5%</strong> of retrieved images highly matched <strong>search intent</strong>.</li>
                <li>‚úÖ <strong>Comparison with Traditional Methods:</strong> Experts unanimously found MCA <strong>more informative and useful</strong> than app-based categorization.</li>
              </ul>
            </p>
          </div>
          <!-- Right Column (Figure) -->
          <div class="column is-half has-text-centered">
            <figure>
              <img src="static/images/allfigures.png" alt="Expert Ratings on the Clusters" style="max-width: 100%; height: auto;">
              <p class="caption-text"><strong>Figure.</strong> The bar plots on the left represent the count of responses for each randomly selected cluster, with varying bar lengths indicating the number of responses and colors representing different slider values. The boxplots with scatter points on the right illustrate the distribution of scores provided by each of the four experts for images within the five cluster topics.</p>
            </figure>
          </div>
        </div>   
        <!-- Full-width figure for Image Retrieval -->
        <div class="has-text-centered">
          <figure>
            <img src="static/images/figure5.png" alt="Expert Ratings on the Image Retrieval" style="max-width: 100%; height: auto;">
            <p class="caption-text"><strong>Figure.</strong> Mean expert ratings for each query, comparing CLIP (green bars) and Llava-OneVision+GTE-Large (purple bars). Higher bars indicate higher relevance scores. Queries are arranged from concrete (e.g., cat) to abstract (e.g., depression).
            </p>
          </figure>
        </div>

      </div>
    </div>
  </div>
</section>
<!-- End section -->

<!-- Key Takeaways -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Key Takeaways</h2>  
        <div class="content has-text-justified">   
          <ul>
            <li>üìä <strong>Capture and Analyze</strong> moment-by-moment screen content</li>
            <li>üîç <strong>Cluster</strong> media based on meaning</li>
            <li>üìë <strong>Automate</strong> topic modeling & labeling</li>
            <li>üñºÔ∏è <strong>Retrieve</strong> images from theoretical queries</li>
            <li>üéõÔ∏è <strong>Explore</strong> media landscapes dynamically</li>
          </ul>  
          <p>
            MCA redefines how researchers study digital media at scale. Unlike traditional methods, it requires 
            <strong>no manual annotation</strong>, safeguards <strong>sensitive data</strong>, and enables scalable, AI-powered 
            exploration of media experiences.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- End section -->

<!-- Contact Section -->
<section class="section has-text-centered">
  <div class="container">
    <h2 class="title is-3">Get in Touch</h2>
    <p class="is-size-5">
      Have questions or want to collaborate? Feel free to reach out!
    </p>
    <br>
    <a href="mailto:mervecer@stanford.edu" class="button is-dark is-medium">
      <span class="icon">
        <i class="fas fa-envelope"></i>
      </span>
      <span>Send an Email</span>
    </a>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{cerit2025mca,
        author = {Merve Cerit and Eric Zelikman and Mu-Jung Cho and Thomas N. Robinson and Byron Reeves and Nilam Ram and Nick Haber},
        title = {Media Content Atlas: A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs},
        booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA '25)},
        year = {2025},
        month = {April},
        location = {Yokohama, Japan},
        publisher = {ACM},
        address = {New York, NY, USA},
        pages = {19},
        doi = {10.1145/3706599.3720055}
      }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
