<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Media Content Atlas</title>
  <link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üì≤üó∫Ô∏è</text></svg>">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Media Content Atlas</h1>
            <h2 class="subtitle is-3">A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=-ziwkNEAAAAJ" target="_blank">Merve Cerit</a>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=en&user=V5B8dSUAAAAJ" target="_blank">Eric Zelikman</a>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=en&user=A_NOSQsAAAAJ" target="_blank">Mu-Jung Cho</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=en&user=UuH5OG8AAAAJ" target="_blank">Thomas N. Robinson</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?hl=en&user=HJWSrNMAAAAJ" target="_blank">Byron Reeves</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=V6mIZuYAAAAJ&hl=en&oi=ao" target="_blank">Nilam Ram</a>,</span>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=euNCoVYAAAAJ&hl=en&oi=ao" target="_blank">Nick Haber</a></span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Stanford University<br>Accepted to <a href="https://chi2025.acm.org" target="_blank">CHI 2025</a></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/mediacontentatlas/mediacontentatlas" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/MCAshort.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        We introduce the Media Content Atlas (MCA)‚Äîa customizable pipeline and accompanying dashboard; powered by open-source multimodal models (e.g., CLIP, LLaVA-One Vision). MCA facilitates content-based clustering, topic modeling, image retrieval, and interactive visualization, enabling open-ended exploration and inductive inquiry. We demonstrate MCA‚Äôs utility with 1.12 million in situ screens from 112 participants, uncovering insights that go far beyond traditional media content categories. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            As digital media use continues to evolve and influence various aspects of life, developing flexible and scalable tools to study complex media experiences is essential. This study introduces the Media Content Atlas (MCA), a novel pipeline designed to help researchers investigate large-scale screen data beyond traditional screen-use metrics. Leveraging multimodal large language models (MLLMs), MCA enables moment-by-moment content analysis, content-based clustering, topic modeling, image retrieval, and interactive visualizations. Evaluated on 1.12 million smartphone screenshots continuously captured during screen use from 112 adults over an entire month, MCA facilitates open-ended exploration and hypothesis generation as well as hypothesis-driven investigations at an unprecedented scale. Expert evaluators underscored its usability and potential for research and intervention design, with clustering results rated 96% relevant and descriptions 83% accurate. By bridging methodological possibilities with domain-specific needs, MCA accelerates both inductive and deductive inquiry, presenting new opportunities for media and HCI research.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->
<!-- Paper Background -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Background</h2>
        <div class="content has-text-justified">
          <p>
            Add problem/background.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper background -->
<!-- Paper Data -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Data</h2>
        <div class="container is-max-desktop">
          <div class="hero-body">
            <img src="static/videos/screenome.gif" alt="Media Content Atlas GIF" style="max-width: 100%; height: auto;">
            <h2 class="subtitle has-text-centered">
              We use data from Human Screenome Project...
            </h2>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper Data -->
<!-- Figures Section -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths has-text-centered">
        <h2 class="title is-3">The MCA Pipeline</h2>
        
        <!-- Large Figure -->
        <figure>
          <img src="static/images/mca-pipeline2.png" alt="Media Content Atlas Pipeline Overview">
          <div class="content has-text-justified">
            <p> MCA pipeline consists of four key steps: <strong>(1)</strong> embedding and description generation, converting complex images into numerical representations and text descriptions; <strong>(2)</strong> clustering and topic modeling to organize content in interpretable topics; <strong>(3)</strong> image retrieval for semantic search; and <strong>(4)</strong> interactive visualization for exploration. This pipeline is designed for scalable and secure analysis of multimodal screen data. We selected models that balance performance and computational efficiency while ensuring all processing remains on in-house servers, safeguarding sensitive data. The pipeline architecture is model-agnostic and can be customized through prompting or fine-tuning. For more information on the models, hyperparameter tuning and prompting, please see <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank">our paper</a>.</p>
        </figure>

      </div>
    </div>
  </div>
</section>
<!-- End Figures Section -->
<!-- Expert Evaluation -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Expert Evaluation & Findings</h2>
        <!-- Two-column layout for Clusters -->
        <div class="columns is-vcentered">
          <!-- Left Column (Text) -->
          <div class="column is-half">
            <p class="content has-text-justified">
              To evaluate MCA‚Äôs clustering, topic labeling and retrieval capabilities, we conducted an assessment with four domain experts specializing in media processes and effects across fields (eg., communication, psychology, medicine, education). Through detailed think-aloud sessions and structured ratings (three hours per expert), they provided feedback on and evaluated:
              <ul>
                <li><strong>Cluster Relevance:</strong> 96% of the topic labels generated for the clusters were rated as <em>relevant or highly relevant</em>.</li>
                <li><strong>Description Accuracy:</strong> 83% of AI-generated content descriptions were considered <em>accurate or highly accurate</em>.</li>
                <strong>Content Similarity:</strong> 89% of images within clusters were deemed <em>similar or highly similar</em>, confirming MCA's effectiveness in grouping related content.</li>
                <li><strong>Image Retrieval Performance:</strong> 79.5% of retrieved images were rated <em>relevant or highly relevant</em> to search queries.</li>
              </ul>
            </p>
          </div>
          <!-- Right Column (Figure) -->
          <div class="column is-half has-text-centered">
            <figure>
              <img src="static/images/allfigures.png" alt="Expert Ratings on the Clusters" style="max-width: 100%; height: auto;">
              <p style="font-size: 0.5em;"></p><p><strong>Figure.</strong> The bar plots on the left represent the count of responses for each randomly selected cluster, with varying bar lengths indicating the number of responses and colors representing different slider values. The boxplots with scatter points on the right illustrate the distribution of scores provided by each of the four experts for images within the five cluster topics.</p>
            </figure>
          </div>
        </div>   
        <!-- Full-width figure for Image Retrieval -->
        <div class="has-text-centered">
          <figure>
            <img src="static/images/figure5.png" alt="Expert Ratings on the Image Retrieval" style="max-width: 100%; height: auto;">
            <figcaption class="has-text-justified">
              <p style="font-size: 0.85em;"></p><p><strong>Figure.</strong> Mean expert ratings for each query, comparing CLIP (green bars) and Llava-OneVision+GTE-Large (purple bars). Higher bars indicate higher relevance scores. Queries are arranged from concrete (e.g., cat) to abstract (e.g., depression). </p>
            </figcaption>
          </figure>
        </div>

      </div>
    </div>
  </div>
</section>

<!-- End section -->
 <!-- Next Steps Evaluation -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths has-text-centered">
        <h2 class="title is-3">Implications</h2>
        
        add bullet points

      </div>
    </div>
  </div>
</section>
<!-- End section -->

<!-- Contact Section -->
<section class="section has-text-centered">
  <div class="container">
    <h2 class="title is-3">Get in Touch</h2>
    <p class="is-size-5">
      Have questions or want to collaborate? Feel free to reach out!
    </p>
    <br>
    <a href="mailto:mervecer@stanford.edu" class="button is-dark is-medium">
      <span class="icon">
        <i class="fas fa-envelope"></i>
      </span>
      <span>Send an Email</span>
    </a>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{cerit2025mca,
        author = {Merve Cerit and Eric Zelikman and Mu-Jung Cho and Thomas N. Robinson and Byron Reeves and Nilam Ram and Nick Haber},
        title = {Media Content Atlas: A Pipeline to Explore and Investigate Multidimensional Media Space using Multimodal LLMs},
        booktitle = {Extended Abstracts of the CHI Conference on Human Factors in Computing Systems (CHI EA '25)},
        year = {2025},
        month = {April},
        location = {Yokohama, Japan},
        publisher = {ACM},
        address = {New York, NY, USA},
        pages = {19},
        doi = {10.1145/3706599.3720055}
      }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the¬†<a href="https://nerfies.github.io" target="_blank">Nerfies</a>¬†project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
